DeepFake Lip-Sync Detection
This is a repository for detecting lip-syncing DeepFakes by spotting temporal inconsistencies between audio and visual data, inspired by the work in "[NeurIPS 2024] Lips Are Lying: Spotting the Temporal Inconsistency between Audio and Visual in Lip-syncing DeepFakes". The repository provides tools for dataset preprocessing, model training, and validation using the AVLips dataset.
Overview
This project implements a novel approach for detecting lip-syncing DeepFakes by analyzing inconsistencies between lip movements and audio signals. It also leverages biological links between lip and head regions to improve detection accuracy. The repository includes scripts for preprocessing the AVLips dataset, training a detection model, and validating its performance.
Requirements
To set up the environment for this project, follow these steps:

Create a Conda environment:
conda create -n LipFD python==3.10
conda activate LipFD


Install dependencies:
pip install -r requirements.txt



Dataset
The project uses the AVLips dataset, a high-quality audio-visual dataset for lip-sync detection containing approximately 340,000 samples generated by state-of-the-art lip-sync methods (e.g., MakeItTalk, Wav2Lip, TalkLip, SadTalker). The dataset is approximately 60 GB after preprocessing.
Download AVLips

Download the AVLips dataset from the provided AVLips v1.0 link (replace with the actual download link).
Extract the dataset and place it in the root directory of the repository.

The dataset folder structure should look like this:
AVLips
├── 0_real
│   ├── 0.mp4
│   └── ...
├── 1_fake
│   ├── 0.mp4
│   └── ...
└── wav
    ├── 0_real
    │   ├── 0.wav
    │   └── ...
    └── 1_fake
        ├── 0.wav
        └── ...

Preprocessing
To preprocess the AVLips dataset for training, run the following command:
python preprocess.py

This script processes the dataset into a format suitable for training. The preprocessed dataset will be organized as follows:
datasets
└── AVLips
    ├── 0_real
    │   ├── 0_0.png
    │   └── ...
    └── 1_fake
        ├── 0_0.png
        └── ...

Note: Preprocessing is optional if you only want to perform validation using the provided validation set.
Validation
To validate the model using pre-trained weights:

Download the pre-trained weights and save them to checkpoints/ckpt.pth.
Download the validation set and extract it to datasets/val.
Run the validation script:python validate.py --real_list_path ./datasets/val/0_real --fake_list_path ./datasets/val/1_fake --ckpt ./checkpoints/ckpt.pth



This will evaluate the model's performance on the validation set.
Training
To train the model from scratch:

Edit the --fake_list_path and --real_list_path in options/base_options.py to point to the preprocessed dataset paths.
Run the training script:python train.py



The model will train on the AVLips dataset and save checkpoints to the checkpoints/ directory.

Notes

The AVLips dataset includes real-world scenarios and applies perturbations (e.g., saturation, contrast, compression, Gaussian noise, blur, and pixelation) to enhance robustness.
The model achieves an average accuracy of 95.3% on lip-syncing detection and up to 90.2% in real-world scenarios (e.g., WeChat video calls).

For more details, refer to the original paper.
